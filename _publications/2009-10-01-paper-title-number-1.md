---
title: "Metrics for Holistic Evaluation of LLM Reasoning about Action, Change, and Planning"
collection: publications
category: papers
permalink: #/publication/2009-10-01-paper-title-number-1
excerpt: 'New informative Metrics for Evaluation of LLM Responses in Planning and Reasoning tasks.'
date: 2025-12-06
venue: 'NeurIPS 2025 Workshop on Evaluating the Evolving LLM Lifecycle: Benchmarks, Emergent Abilities, and Scaling'
slidesurl: #'http://academicpages.github.io/files/slides1.pdf'
paperurl: #'http://academicpages.github.io/files/paper1.pdf'
bibtexurl: #'http://academicpages.github.io/files/bibtex1.bib'
citation: #'Your Name, You. (2009). &quot;Paper Title Number 1.&quot; <i>Journal 1</i>. 1(1).'
---
Planning, reasoning, and sequential decision-making have played a pivotal role in the development of AI systems. While Large Language Models (LLMs) have demonstrated impressive capabilities, their evaluation for planning and Reasoning about Action and Change (RAC) problems is performed using strict binary success criteria, which limits information for further analysis and development. Given the probabilistic and autoregressive nature of LLMs, this work proposes the use of simple non-binary task-specific metrics for the evaluation of LLM responses for planning and reasoning tasks that go beyond perfect matching with ground truth, by utilizing set comparison methods, while still maintaining rigid and non-malleable evaluation criteria. We demonstrate the utility and usefulness of this type of metric in obtaining richer data fidelity and information about the quality, precision, nature of LLMs' responses, and their closeness to the ground truth through evaluations on six different tasks across two domains. With two case study examples, we additionally demonstrate the feasibility of comparative analysis of different task-specific data distributions obtained through this metric.
